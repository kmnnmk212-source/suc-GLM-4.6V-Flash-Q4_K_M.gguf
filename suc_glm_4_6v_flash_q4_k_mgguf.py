# -*- coding: utf-8 -*-
"""suc_GLM-4.6V-Flash-Q4_K_Mgguf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YyZCVlocX60UDm8ccmSD6k4Bum_gvXwY
"""





"""https://huggingface.co/zai-org/GLM-4.6V-Flash/discussions/21"""

!wget https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_M.gguf

# using CLI
llama-cli -hf ggml-org/GLM-4.6V-Flash-GGUF

# or, deploy as server
llama-server -hf ggml-org/GLM-4.6V-Flash-GGUF

!wget https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz

!unzip llama-b7593-bin-ubuntu-x64.tar.gz

!wget https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz

!tar -xzvf /content/llama-b7593-bin-ubuntu-x64.tar.gz

!tar -xzvf llama-b7593-bin-ubuntu-x64.tar.gz

!ls -F

!./llama-b7593/llama-cli -h

!./llama-b7593/llama-cli -hf ggml-org/GLM-4.6V-Flash-GGUF

"""https://huggingface.co/zai-org/GLM-4.6V-Flash/discussions/21"""

!./llama-b7593/llama-cli -m /content/GLM-4.6V-Flash-Q4_K_M.gguf

!./llama-b7593/llama-cli -m /content/GLM-4.6V-Flash-Q4_K_M.gguf

