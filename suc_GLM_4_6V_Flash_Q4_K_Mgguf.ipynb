{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJxuIaYztv-w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MhH8m1zxA61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/zai-org/GLM-4.6V-Flash/discussions/21"
      ],
      "metadata": {
        "id": "8JTlSGARxZtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvlPL9H8xA9k",
        "outputId": "1a30e490-6c5c-4321-bf60-c7fffdb6af24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-31 18:55:58--  https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.94.27, 108.138.94.97, 108.138.94.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.94.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/69377a4eea5905958a31c423/1bc2efb9a1b5f9e07e59f170f67029eb567300acaf67079eec85a59470859927?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251231%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251231T185558Z&X-Amz-Expires=3600&X-Amz-Signature=a55189657614f0d7268a0519d3b084899cfb38f341da5bf2c62d71ccfbc67fe9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GLM-4.6V-Flash-Q4_K_M.gguf%3B+filename%3D%22GLM-4.6V-Flash-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1767210958&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIxMDk1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTM3N2E0ZWVhNTkwNTk1OGEzMWM0MjMvMWJjMmVmYjlhMWI1ZjllMDdlNTlmMTcwZjY3MDI5ZWI1NjczMDBhY2FmNjcwNzllZWM4NWE1OTQ3MDg1OTkyNyoifV19&Signature=i%7E-lXe1KmmgGW%7EVN%7Es5I%7E5jQdVnJMz4SHGxfC5Z9UbPVxhnXJKlGY7TgMFfY0vhv7u0N6rKpMLnQ5rWq1i9N5Pdz7zUrKLDOY1tpwJUCRMV6vH6O6K5h56frIF7qXL72N5-bjKjZNVxmXoFabmDJ8TcA9Y6LrYP1ngqHWp4-AaD6smX0OSE6T%7Ey32UvV0Bn%7EXRzL7bZU4uaaPFIsjEQuBTWmf4Yl-ZvdWzhD-RhF2n5HrMTcXHyHoieCKkUj51nkOqJh9dsGIRERFqcUTZT8ekqzKjOyHhs1nEIpY9SJh-I7TQ4doxRipPhYwJprLV7c23oBN2-JrraT8V2SQyOqAQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-31 18:55:58--  https://cas-bridge.xethub.hf.co/xet-bridge-us/69377a4eea5905958a31c423/1bc2efb9a1b5f9e07e59f170f67029eb567300acaf67079eec85a59470859927?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251231%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251231T185558Z&X-Amz-Expires=3600&X-Amz-Signature=a55189657614f0d7268a0519d3b084899cfb38f341da5bf2c62d71ccfbc67fe9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GLM-4.6V-Flash-Q4_K_M.gguf%3B+filename%3D%22GLM-4.6V-Flash-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1767210958&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIxMDk1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTM3N2E0ZWVhNTkwNTk1OGEzMWM0MjMvMWJjMmVmYjlhMWI1ZjllMDdlNTlmMTcwZjY3MDI5ZWI1NjczMDBhY2FmNjcwNzllZWM4NWE1OTQ3MDg1OTkyNyoifV19&Signature=i%7E-lXe1KmmgGW%7EVN%7Es5I%7E5jQdVnJMz4SHGxfC5Z9UbPVxhnXJKlGY7TgMFfY0vhv7u0N6rKpMLnQ5rWq1i9N5Pdz7zUrKLDOY1tpwJUCRMV6vH6O6K5h56frIF7qXL72N5-bjKjZNVxmXoFabmDJ8TcA9Y6LrYP1ngqHWp4-AaD6smX0OSE6T%7Ey32UvV0Bn%7EXRzL7bZU4uaaPFIsjEQuBTWmf4Yl-ZvdWzhD-RhF2n5HrMTcXHyHoieCKkUj51nkOqJh9dsGIRERFqcUTZT8ekqzKjOyHhs1nEIpY9SJh-I7TQ4doxRipPhYwJprLV7c23oBN2-JrraT8V2SQyOqAQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.63, 18.238.217.126, 18.238.217.64, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6166578784 (5.7G)\n",
            "Saving to: ‘GLM-4.6V-Flash-Q4_K_M.gguf’\n",
            "\n",
            "GLM-4.6V-Flash-Q4_K 100%[===================>]   5.74G  89.4MB/s    in 34s     \n",
            "\n",
            "2025-12-31 18:56:32 (174 MB/s) - ‘GLM-4.6V-Flash-Q4_K_M.gguf’ saved [6166578784/6166578784]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using CLI\n",
        "llama-cli -hf ggml-org/GLM-4.6V-Flash-GGUF\n",
        "\n",
        "# or, deploy as server\n",
        "llama-server -hf ggml-org/GLM-4.6V-Flash-GGUF\n"
      ],
      "metadata": {
        "id": "Jk5zTIPzxDLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtigkQPqxuud",
        "outputId": "e859c8ae-7680-4d7c-dde2-73b9e5a44929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-31 18:59:01--  https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/62d6e762-d22d-4b87-ae5b-e76fc27496da?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-31T19%3A49%3A46Z&rscd=attachment%3B+filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-31T18%3A49%3A44Z&ske=2025-12-31T19%3A49%3A46Z&sks=b&skv=2018-11-09&sig=YvSaSbi%2FLaG2YInJZOFk7dPlKo5jUjeim29gElv8LlE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzIwOTM0MSwibmJmIjoxNzY3MjA3NTQxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3_l41abdOQee3BlnToZ04m1YsZq-_UTQlPyAZsSa0WY&response-content-disposition=attachment%3B%20filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-31 18:59:01--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/62d6e762-d22d-4b87-ae5b-e76fc27496da?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-31T19%3A49%3A46Z&rscd=attachment%3B+filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-31T18%3A49%3A44Z&ske=2025-12-31T19%3A49%3A46Z&sks=b&skv=2018-11-09&sig=YvSaSbi%2FLaG2YInJZOFk7dPlKo5jUjeim29gElv8LlE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzIwOTM0MSwibmJmIjoxNzY3MjA3NTQxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3_l41abdOQee3BlnToZ04m1YsZq-_UTQlPyAZsSa0WY&response-content-disposition=attachment%3B%20filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21521773 (21M) [application/octet-stream]\n",
            "Saving to: ‘llama-b7593-bin-ubuntu-x64.tar.gz’\n",
            "\n",
            "llama-b7593-bin-ubu 100%[===================>]  20.52M   107MB/s    in 0.2s    \n",
            "\n",
            "2025-12-31 18:59:02 (107 MB/s) - ‘llama-b7593-bin-ubuntu-x64.tar.gz’ saved [21521773/21521773]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b7593-bin-ubuntu-x64.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dROLv9kxxpy",
        "outputId": "abff4a2c-7b63-46de-d8d2-ed36108502cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b7593-bin-ubuntu-x64.tar.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of llama-b7593-bin-ubuntu-x64.tar.gz or\n",
            "        llama-b7593-bin-ubuntu-x64.tar.gz.zip, and cannot find llama-b7593-bin-ubuntu-x64.tar.gz.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8EX2WrgxznD",
        "outputId": "65243106-e5f3-4ce0-e613-64811a089356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-31 19:00:05--  https://github.com/ggml-org/llama.cpp/releases/download/b7593/llama-b7593-bin-ubuntu-x64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/62d6e762-d22d-4b87-ae5b-e76fc27496da?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-31T19%3A49%3A46Z&rscd=attachment%3B+filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-31T18%3A49%3A44Z&ske=2025-12-31T19%3A49%3A46Z&sks=b&skv=2018-11-09&sig=YvSaSbi%2FLaG2YInJZOFk7dPlKo5jUjeim29gElv8LlE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzIwOTM0MSwibmJmIjoxNzY3MjA3NTQxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3_l41abdOQee3BlnToZ04m1YsZq-_UTQlPyAZsSa0WY&response-content-disposition=attachment%3B%20filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-31 19:00:05--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/62d6e762-d22d-4b87-ae5b-e76fc27496da?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-31T19%3A49%3A46Z&rscd=attachment%3B+filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-31T18%3A49%3A44Z&ske=2025-12-31T19%3A49%3A46Z&sks=b&skv=2018-11-09&sig=YvSaSbi%2FLaG2YInJZOFk7dPlKo5jUjeim29gElv8LlE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzIwOTM0MSwibmJmIjoxNzY3MjA3NTQxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3_l41abdOQee3BlnToZ04m1YsZq-_UTQlPyAZsSa0WY&response-content-disposition=attachment%3B%20filename%3Dllama-b7593-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21521773 (21M) [application/octet-stream]\n",
            "Saving to: ‘llama-b7593-bin-ubuntu-x64.tar.gz.1’\n",
            "\n",
            "\r          llama-b75   0%[                    ]       0  --.-KB/s               \rllama-b7593-bin-ubu 100%[===================>]  20.52M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-31 19:00:05 (193 MB/s) - ‘llama-b7593-bin-ubuntu-x64.tar.gz.1’ saved [21521773/21521773]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf /content/llama-b7593-bin-ubuntu-x64.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U0PyH6kyBMi",
        "outputId": "aaf7e912-267e-4fbc-8890-4afade0fdd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/llama-b7593-bin-ubuntu-x64.tar.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/llama-b7593-bin-ubuntu-x64.tar.gz or\n",
            "        /content/llama-b7593-bin-ubuntu-x64.tar.gz.zip, and cannot find /content/llama-b7593-bin-ubuntu-x64.tar.gz.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c192b3f2",
        "outputId": "51ea3317-28d5-4eeb-8c58-05abd5104263"
      },
      "source": [
        "!tar -xzvf llama-b7593-bin-ubuntu-x64.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./\n",
            "llama-b7593/libggml.so\n",
            "llama-b7593/libmtmd.so\n",
            "llama-b7593/libmtmd.so.0\n",
            "llama-b7593/llama-fit-params\n",
            "llama-b7593/llama-llava-cli\n",
            "llama-b7593/libggml-cpu-cascadelake.so\n",
            "llama-b7593/rpc-server\n",
            "llama-b7593/LICENSE\n",
            "llama-b7593/libggml-cpu-x64.so\n",
            "llama-b7593/llama-imatrix\n",
            "llama-b7593/libggml-cpu-sse42.so\n",
            "llama-b7593/LICENSE-jsonhpp\n",
            "llama-b7593/libggml-cpu-icelake.so\n",
            "llama-b7593/llama-quantize\n",
            "llama-b7593/libggml.so.0.9.4\n",
            "llama-b7593/libggml-cpu-haswell.so\n",
            "llama-b7593/libggml-base.so.0.9.4\n",
            "llama-b7593/libggml-cpu-sapphirerapids.so\n",
            "llama-b7593/llama-server\n",
            "llama-b7593/libggml-base.so\n",
            "llama-b7593/LICENSE-linenoise\n",
            "llama-b7593/libggml-cpu-piledriver.so\n",
            "llama-b7593/libggml-cpu-sandybridge.so\n",
            "llama-b7593/llama-tokenize\n",
            "llama-b7593/llama-cli\n",
            "llama-b7593/libggml-cpu-zen4.so\n",
            "llama-b7593/llama-qwen2vl-cli\n",
            "llama-b7593/llama-batched-bench\n",
            "llama-b7593/libllama.so.0\n",
            "llama-b7593/libllama.so\n",
            "llama-b7593/llama-gguf-split\n",
            "llama-b7593/libggml-rpc.so\n",
            "llama-b7593/llama-perplexity\n",
            "llama-b7593/libggml-cpu-skylakex.so\n",
            "llama-b7593/llama-gemma3-cli\n",
            "llama-b7593/libggml-cpu-cannonlake.so\n",
            "llama-b7593/llama-tts\n",
            "llama-b7593/libmtmd.so.0.0.7593\n",
            "llama-b7593/libggml-cpu-ivybridge.so\n",
            "llama-b7593/libllama.so.0.0.7593\n",
            "llama-b7593/LICENSE-curl\n",
            "llama-b7593/LICENSE-httplib\n",
            "llama-b7593/libggml.so.0\n",
            "llama-b7593/llama-completion\n",
            "llama-b7593/libggml-cpu-alderlake.so\n",
            "llama-b7593/llama-minicpmv-cli\n",
            "llama-b7593/libggml-base.so.0\n",
            "llama-b7593/llama-mtmd-cli\n",
            "llama-b7593/llama-run\n",
            "llama-b7593/llama-bench\n",
            "llama-b7593/libggml-cpu-cooperlake.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "910ec37d",
        "outputId": "1a4585f0-1db2-4d21-db92-94e496fe2581"
      },
      "source": [
        "!ls -F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLM-4.6V-Flash-Q4_K_M.gguf\t   llama-b7593-bin-ubuntu-x64.tar.gz.1\n",
            "llama-b7593-bin-ubuntu-x64.tar.gz  sample_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-b7593/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcxVKfCwyEMx",
        "outputId": "a410af02-3d82-4b45-b21a-47550c965037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7593/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7593/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "-cl,   --cache-list                     show list of models in cache\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of CPU threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "-fa,   --flash-attn [on|off|auto]       set Flash Attention use ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--perf, --no-perf                       whether to enable internal libllama performance timings (default:\n",
            "                                        false)\n",
            "                                        (env: LLAMA_ARG_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape, --no-escape            whether to process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "                                        (default: true)\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-kvo,  --kv-offload, -nkvo, --no-kv-offload\n",
            "                                        whether to enable KV cache offloading (default: enabled)\n",
            "                                        (env: LLAMA_ARG_KV_OFFLOAD)\n",
            "--repack, -nr, --no-repack              whether to enable weight repacking (default: enabled)\n",
            "                                        (env: LLAMA_ARG_REPACK)\n",
            "--no-host                               bypass host buffer allowing extra buffers to be used\n",
            "                                        (env: LLAMA_ARG_NO_HOST)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers (host:port)\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--mmap, --no-mmap                       whether to memory-map model (if disabled, slower load but may reduce\n",
            "                                        pageouts if not using mlock) (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "-ot,   --override-tensor <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "                                        (env: LLAMA_ARG_OVERRIDE_TENSOR)\n",
            "-cmoe, --cpu-moe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "-ncmoe, --n-cpu-moe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   max. number of layers to store in VRAM, either an exact number,\n",
            "                                        'auto', or 'all' (default: auto)\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "-fit,  --fit [on|off]                   whether to adjust unset arguments to fit in device memory ('on' or\n",
            "                                        'off', default: 'on')\n",
            "                                        (env: LLAMA_ARG_FIT)\n",
            "-fitt, --fit-target MiB                 target margin per device for --fit option, default: 1024\n",
            "                                        (env: LLAMA_ARG_FIT_TARGET)\n",
            "-fitc, --fit-ctx N                      minimum ctx size that can be set by --fit option, default: 4096\n",
            "                                        (env: LLAMA_ARG_FIT_CTX)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE,...        advanced option to override model metadata by key. to specify multiple\n",
            "                                        overrides, either use comma-separated or repeat this argument.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false,tokenizer.ggml.add_eos_token=bool:false\n",
            "--op-offload, --no-op-offload           whether to offload host tensor operations to device (default: true)\n",
            "--lora FNAME                            path to LoRA adapter (use comma-separated values to load multiple\n",
            "                                        adapters)\n",
            "--lora-scaled FNAME:SCALE,...           path to LoRA adapter with user defined scaling (format:\n",
            "                                        FNAME:SCALE,...)\n",
            "                                        note: use comma-separated values\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: use comma-separated values to add multiple control vectors\n",
            "--control-vector-scaled FNAME:SCALE,...\n",
            "                                        add a control vector with user defined scaling SCALE\n",
            "                                        note: use comma-separated values (format: FNAME:SCALE,...)\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path to load\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-dr,   --docker-repo [<repo>/]<model>[:quant]\n",
            "                                        Docker Hub model repository. repo is optional, default to ai/. quant\n",
            "                                        is optional, default to :latest.\n",
            "                                        example: gemma3\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_DOCKER_REPO)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "                                        (env: LLAMA_LOG_FILE)\n",
            "--log-colors [on|off|auto]              Set colored logging ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored. Values:\n",
            "                                         - 0: generic output\n",
            "                                         - 1: error\n",
            "                                         - 2: warning\n",
            "                                         - 3: info\n",
            "                                         - 4: debug\n",
            "                                        (default: 1)\n",
            "                                        \n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampler-seq, --sampling-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "                                        (env: LLAMA_ARG_TOP_K)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--display-prompt, --no-display-prompt   whether to print prompt at generation (default: true)\n",
            "-co,   --color [on|off|auto]            Colorize output to distinguish prompt and user input from generations\n",
            "                                        ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "--ctx-checkpoints, --swa-checkpoints N\n",
            "                                        max number of context checkpoints to create per slot (default:\n",
            "                                        8)[(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)\n",
            "                                        (env: LLAMA_ARG_CTX_CHECKPOINTS)\n",
            "-cram, --cache-ram N                    set the maximum cache size in MiB (default: 8192, -1 - no limit, 0 -\n",
            "                                        disable)[(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/16391)\n",
            "                                        (env: LLAMA_ARG_CACHE_RAM)\n",
            "--context-shift, --no-context-shift     whether to use context shift on infinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "--show-timings, --no-show-timings       whether to show timing information after each response (default: true)\n",
            "                                        (env: LLAMA_ARG_SHOW_TIMINGS)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation, -no-cnv, --no-conversation\n",
            "                                        whether to run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--warmup, --no-warmup                   whether to perform warmup with an empty run (default: enabled)\n",
            "-mm,   --mmproj FILE                    path to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        note: if -hf is used, this argument can be omitted\n",
            "                                        (env: LLAMA_ARG_MMPROJ)\n",
            "-mmu,  --mmproj-url URL                 URL to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        (env: LLAMA_ARG_MMPROJ_URL)\n",
            "--mmproj-auto, --no-mmproj, --no-mmproj-auto\n",
            "                                        whether to use multimodal projector file (if available), useful when\n",
            "                                        using -hf (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_AUTO)\n",
            "--mmproj-offload, --no-mmproj-offload   whether to enable GPU offloading for multimodal projector (default:\n",
            "                                        enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_OFFLOAD)\n",
            "--image, --audio FILE                   path to an image or audio file. use with multimodal models, use\n",
            "                                        comma-separated values for multiple files\n",
            "--image-min-tokens N                    minimum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MIN_TOKENS)\n",
            "--image-max-tokens N                    maximum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MAX_TOKENS)\n",
            "-otd,  --override-tensor-draft <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type for draft model\n",
            "-cmoed, --cpu-moe-draft                 keep all Mixture of Experts (MoE) weights in the CPU for the draft\n",
            "                                        model\n",
            "                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)\n",
            "-ncmoed, --n-cpu-moe-draft N            keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU for the draft model\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)\n",
            "--chat-template-kwargs STRING           sets additional params for the json template parser\n",
            "                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)\n",
            "--jinja, --no-jinja                     whether to use jinja template engine for chat (default: enabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content`\n",
            "                                        - deepseek-legacy: keeps `<think>` tags in `message.content` while\n",
            "                                        also populating `message.reasoning_content`\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "--draft, --draft-n, --draft-max N       number of tokens to draft for speculative decoding (default: 16)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MAX)\n",
            "--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding\n",
            "                                        (default: 0)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MIN)\n",
            "--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)\n",
            "                                        (env: LLAMA_ARG_DRAFT_P_MIN)\n",
            "-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded\n",
            "                                        from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)\n",
            "-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model\n",
            "                                        (none = don't offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "-ngld, --gpu-layers-draft, --n-gpu-layers-draft N\n",
            "                                        max. number of draft model layers to store in VRAM, either an exact\n",
            "                                        number, 'auto', or 'all' (default: auto)\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)\n",
            "-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_DRAFT)\n",
            "--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main\n",
            "                                        model are not compatible\n",
            "--gpt-oss-20b-default                   use gpt-oss-20b (note: can download weights from the internet)\n",
            "--gpt-oss-120b-default                  use gpt-oss-120b (note: can download weights from the internet)\n",
            "--vision-gemma-4b-default               use Gemma 3 4B QAT (note: can download weights from the internet)\n",
            "--vision-gemma-12b-default              use Gemma 3 12B QAT (note: can download weights from the internet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-b7593/llama-cli -hf ggml-org/GLM-4.6V-Flash-GGUF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbqAVZLxy48M",
        "outputId": "181eff4e-5bc3-4fc2-b0eb-a0af0ab9e86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7593/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7593/libggml-cpu-haswell.so\n",
            "common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/ggml-org_GLM-4.6V-Flash-GGUF_GLM-4.6V-Flash-Q4_K_M.gguf\n",
            "common_download_file_single_online: trying to download model from https://huggingface.co/ggml-org/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_M.gguf to /root/.cache/llama.cpp/ggml-org_GLM-4.6V-Flash-GGUF_GLM-4.6V-Flash-Q4_K_M.gguf.downloadInProgress (server_etag:\"1035ddf1a990059e5c649f2f468b08e8f670a86ad128ff9944f8059155cf7e80\", server_last_modified:)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1339  100  1339    0     0  13158      0 --:--:-- --:--:-- --:--:-- 13158\n",
            " 33 5880M   33 1943M    0     0   113M      0  0:00:51  0:00:17  0:00:34  137M^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/zai-org/GLM-4.6V-Flash/discussions/21"
      ],
      "metadata": {
        "id": "XBNGToNgzNzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-b7593/llama-cli -m /content/GLM-4.6V-Flash-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxe_L8d5zF2p",
        "outputId": "f1c35171-def6-4e09-d1f3-2470a012b252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7593/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7593/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7593-9b8329de7\n",
            "model      : GLM-4.6V-Flash-Q4_K_M.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> /\b/\b/read  1.txt\n",
            "\u001b[0m\n",
            "Loaded text from '1.txt'\n",
            "\u001b[1m\u001b[32m\n",
            "> translate text in the text file into arabic\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b \b\u001b[90m[Start thinking]\n",
            "The user wants me to translate the provided text from Chapter 1 of \"Dracula\" into Arabic.\n",
            "\n",
            "1.  **Analyze the Request:** The user has provided a specific text snippet and wants it translated into Arabic.\n",
            "    *   **Source Text:** \"CHAPTER 1 Jonathan Harker's Journal 3 May. Bistritz.--Left Munich at 8:35 P.M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little I could walk through the streets. I feared to go very far from the station, as we had arrived late and would start as near the correct time as possible. The impression I had was that we were leaving the West and entering the East; the most western of splendid bridges over the Danube, which is here of noble width and depth, took us among the traditions of Turkish rule.\"\n",
            "    *   **Target Language:** Arabic.\n",
            "\n",
            "2.  **Initial Translation Strategy:**\n",
            "    *   I'll go through the text sentence by sentence.\n",
            "    *   I need to be careful with names, dates, times, and place names (Bistritz, Munich, Vienna, Buda-Pesth, Danube).\n",
            "    *   The tone is formal and diary-like, so the Arabic translation should reflect that. It's a journal entry, so a slightly formal, narrative style is appropriate.\n",
            "    *   I should consider using the Arabic alphabet and the standard Arabic script. I'll write it out phonetically or in the correct Arabic script. The user's request is for \"translate text in the text file into arabic,\" which implies I should output the Arabic text.\n",
            "\n",
            "3.  **Sentence-by-Sentence Translation:**\n",
            "\n",
            "    *   **\"CHAPTER 1 Jonathan Harker's Journal\"**\n",
            "        *   \"CHAPTER 1\" -> \"الفصل الأول\" (al-fasl al-awwal).\n",
            "        *   \"Jonathan Harker's Journal\" -> \"مذكرات جوناثان هاركر\" (mukhakirat jonathan harker). The possessive \"'s\" becomes 's. \"Jonathan Harker\" is a proper name, so it should be transliterated and used as is in Arabic.\n",
            "\n",
            "    *   **\"3 May. Bistritz.--\"**\n",
            "        *   \"3 May\" -> \"3 مايو\" (3 Mayu).\n",
            "        *   \"Bistritz\" -> This is a proper name, place name. It should be transliterated or transliterated and kept as is. \"بيستريتس\" (Bistrits) or \"بيسترِيتس\" (Bistrits). \"Bistritz\" is more likely the standard way to write it. I'll go with \"بيسترِيتس\".\n",
            "        *   The em dash \"--\" is a punctuation mark. In Arabic, we can represent it with \"ـ\" or \"–\". \"ـ\" is more common for this type of punctuation in formal texts. So, \"3 مايو. بيسترِيتس.--\".\n",
            "\n",
            "    *   **\"Left Munich at 8:35 P.M., on 1st May,\"**\n",
            "        *   \"Left\" -> \"تغادرنا\" (tagadirna), \"تركنا\" (takruna), or \"غادرنا\" (ghadurna). \"غادرنا\" (left we) or \"تركنا\" (left) are good options. \"غادرنا\" is more formal for a past event. Let's stick with \"غادرنا\".\n",
            "        *   \"Munich\" -> \"ميونيخ\" (Munyakha).\n",
            "        *   \"at 8:35 P.M.\" -> \"في الساعة 8:35 مساءً\" (fi as-saa 8:35 masaa'an). \"P.M.\" is \"مساءً\" (masaa'an) in Arabic.\n",
            "        *   \"on 1st May\" -> \"في 1 مايو\" (fi 1 Mayu).\n",
            "        *   Putting it together: \"غادرنا ميونيخ في الساعة 8:35 مساءً، في 1 مايو\". The comma after \"May\" is important.\n",
            "\n",
            "    *   **\"arriving at Vienna early next morning;\"**\n",
            "        *   \"arriving at\" -> \"وصلنا إلى\" (wasla na 'ala). \"وصلنا\" (we arrived) + \"إلى\" (to).\n",
            "        *   \"Vienna\" -> \"فيينا\" (Viyana).\n",
            "        *   \"early next morning\" -> \"في الصباح المقبل مبكراً\" (fi as-sabah al-muqallib minbara). \"Early next morning\" translates to \"الصباح المقبل مبكراً\" or \"في الصباح المقبل مبكراً\". The latter is more common for a description.\n",
            "        *   Putting it together: \"وصلنا إلى فيينا مبكراً في الصباح المقبل\". The semicolon is important.\n",
            "\n",
            "    *   **\"should have arrived at 6:46, but train was an hour late.\"**\n",
            "        *   \"should have arrived\" -> \"كان يجب أن نصل في الساعة 6:46\" (kana ijib an nasala fi as-saa 6:46). The \"should have\" structure is translated with \"كان يجب أن...\". \"had to arrive\" is \"كانا مقصورين على...\". \"should have arrived\" is \"كانا يجب أن يصل...\". So \"كان يجب أن نصل في الساعة 6:46\".\n",
            "        *   \"but\" -> \"ولكن\" (wala).\n",
            "        *   \"train was an hour late\" -> \"كان العجلة (or the train) متأخرة بساعة واحدة\" (kan al-'ajl (or al-ta'arruq) muta'akhirta b-saa' wahida). \"Train\" is \"القطار\" (al-qitara). So \"كان القطار متأخر بساعة واحدة\".\n",
            "        *   Putting it together: \"كان يجب أن نصل في الساعة\n",
            "\u001b[35m\n",
            "[ Prompt: 2.2 t/s | Generation: 0.9 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> \u001b[0m\n",
            "/content/llama-b7593/libggml-base.so.0(+0x1844b)[0x79811090d44b]\n",
            "/content/llama-b7593/libggml-base.so.0(ggml_print_backtrace+0x21f)[0x79811090d8af]\n",
            "/content/llama-b7593/libggml-base.so.0(+0x2bbdf)[0x798110920bdf]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c)[0x79811077520c]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277)[0x798110775277]\n",
            "./llama-b7593/llama-cli(+0x277e71)[0x59af37e00e71]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x45495)[0x7981103dc495]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7981103dc610]\n",
            "./llama-b7593/llama-cli(+0x7134f)[0x59af37bfa34f]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7981103d9520]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x91117)[0x798110428117]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x96624)[0x79811042d624]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6thread4joinEv+0x17)[0x7981107a32c7]\n",
            "./llama-b7593/llama-cli(+0x277397)[0x59af37e00397]\n",
            "./llama-b7593/llama-cli(+0x239a1d)[0x59af37dc2a1d]\n",
            "./llama-b7593/llama-cli(+0x6e54a)[0x59af37bf754a]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7981103c0d90]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7981103c0e40]\n",
            "./llama-b7593/llama-cli(+0x711c5)[0x59af37bfa1c5]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-b7593/llama-cli -m /content/GLM-4.6V-Flash-Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "FQP2p-nY0Fhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53089ebb-c671-4973-afbe-fd992d225172"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7593/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7593/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7593-9b8329de7\n",
            "model      : GLM-4.6V-Flash-Q4_K_M.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> /\b/\b/read 1.txt\n",
            "\u001b[0m\n",
            "Loaded text from '1.txt'\n",
            "\u001b[1m\u001b[32m\n",
            "> what is in the text file?\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b \b\u001b[90m[Start thinking]\n",
            "The user is asking about what is in the text file. The user provided a snippet from \"Chapter 1: Jonathan Harker's Journal\" from \"Dracula\". So, the text file contains this chapter.\n",
            "\n",
            "I need to provide the content of the text file as requested. The user has provided the first part of the text. So I will include the entire text provided by the user.\n",
            "\n",
            "The text is:\n",
            "```\n",
            "CHAPTER 1\n",
            "Jonathan Harker's Journal\n",
            "3 May. Bistritz.--Left Munich at 8:35 P.M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late.\n",
            "Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little I could walk through the streets. I feared to go very far from the station, as we had arrived late and would start as near the correct time as possible.\n",
            "The impression I had was that we were leaving the West and entering the East; the most western of splendid bridges over the Danube, which is here of noble width and depth, took us among the traditions of Turkish rule.\n",
            "```\n",
            "\n",
            "So the answer is the content of the text file. I will present it clearly.\n",
            "\n",
            "The question is \"what is in the text file?\". So I need to output the text content.\n",
            "\n",
            "I should not add any additional interpretation unless asked. Just present the content.\n",
            "\n",
            "Okay.\n",
            "[End thinking]\n",
            "\n",
            "\u001b[0mThe text file contains the following content:\n",
            "\n",
            "```\n",
            "CHAPTER 1\n",
            "Jonathan Harker's Journal\n",
            "3 May. Bistritz.--Left Munich at 8:35 P.M., on 1st May, arriving at Vienna\n",
            "early next morning; should have arrived at 6:46, but train was an hour late.\n",
            "Buda-Pesth seems a wonderful place, from\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFyYUOjq8Zuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}